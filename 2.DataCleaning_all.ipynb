{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning of Text Reviews \n",
    "\n",
    "\n",
    "### Techniques\n",
    "1. Tranform all reviews to lower case \n",
    "2. Remove special characters and punctuations \n",
    "3. Remove numbers \n",
    "4. Remove stop words \n",
    "5. Remove words with repeating characters \n",
    "6. Apply Part of Speech (POS) tagging and lemmatization to get root words \n",
    "\n",
    "### Output \n",
    "Dataframe with cleaned text reviews for further analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "TxY0wiCAE9Tq",
    "outputId": "367e2dd2-cc16-448b-e048-ad56b6d07848"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Tea made with Lipton Yellow Label teabags is m...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this tea!  Okay, I'm not a high falutin...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Discovered this tea at a local Med. Rest....a ...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Well I bought this tea after being in Malaysia...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>We really like this tea.  It is definitely dif...</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText department\n",
       "0      5.0                                No adverse comment.  Beverages\n",
       "1      5.0                          Gift for college student.  Beverages\n",
       "2      5.0  If you like strong tea, this is for you. It mi...  Beverages\n",
       "3      5.0  Love the tea. The flavor is way better than th...  Beverages\n",
       "4      5.0  I have searched everywhere until I browsed Ama...  Beverages\n",
       "5      4.0  Tea made with Lipton Yellow Label teabags is m...  Beverages\n",
       "6      5.0  I love this tea!  Okay, I'm not a high falutin...  Beverages\n",
       "7      5.0  Discovered this tea at a local Med. Rest....a ...  Beverages\n",
       "8      4.0  Well I bought this tea after being in Malaysia...  Beverages\n",
       "9      5.0  We really like this tea.  It is definitely dif...  Beverages"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"reviews.pkl\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join al the reviews as a body of text \n",
    "\n",
    "text_old = \"\".join(r for r in data['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters include: {'+', '.', '\\x03', '}', '&', '/', '<', '\\x11', ')', '#', '\\x08', \"'\", '{', '|', ';', '?', '\\x10', '`', ']', ',', '-', ':', '=', '%', '$', '\\n', '\"', '[', '^', '@', '~', '!', '_', '\\x7f', '\\x1b', '*', '(', '\\\\', '>'}\n",
      "Repeating characters include: {'RRRRRR', 'OOOOOOOOOOOOOOOOOOO', 'eeeeeeeee', 'GGG', 'hhhhhhhhh', 'nnnn', 'sssssssss', 'ssssssssssss', '1111', 'OOOOOOO', 'kkkk', 'AAAAAAAA', 'QQ', 'OO', 'dddddd', 'ooooooooooooooooooo', 'AA', 'AAAAAAAAAAAAAAAAAAA', 'eeeee', 'llll', 'uuuuu', 'SSSSSS', 'hhhhhhh', 'VVV', 'UUU', 'fff', 'IIIIIIII', 'aaaaa', 'nnn', 'RRRRRRRRRRRRRRRRRRR', 'hhhhhh', 'wwwwwwwwwwwwwwww', 'nnnnn', 'rrrrr', 'rrrrrrrrrrr', 'vvvvvvvvv', '____________', 'GG', 'sssssss', '0000000000', 'WW', 'nnnnnnnn', 'lllllllll', 'EEEEEEEEEEE', 'hhhhhhhhhhhhhhhhhhhhhhhh', 'ssss', '___________', 'OOO', 'yyyy', '_______________________________________________________________________________________________________', 'HHHH', 'zzzzzzzz', 'ZZZ', 'zzz', 'ooooooo', 'oooooooooooo', 'OOOOOOOOOOOOOOOOO', 'MMMMMMMMMMMMMMM', 'PPP', 'uuuuuuuuuuuuuuuuuu', 'hhhhh', 'aaaaaaaaaaaaaaaa', 'hhhhhhhhhhhhh', '999', 'MMMMMMMMMMMMMMMM', 'III', 'iiii', 'eeeeeeeeee', 'rrrrrrrrr', 'hhhhhhhhhh', 'GGGGGG', '22', 'oooooooooo', '___________________', 'OOOO', 'tt', 'mm', 'LLLLLL', 'CC', 'ffffffffffffff', 'RR', 'RRRRRRRR', '_________', 'OOOOOOOOOOOOOOOO', 'wwwwwwww', 'rrrrrrrrrr', '_________________', 'TTTTTT', '________________________________________________________________________________', 'mmmmmmmmmmmmmmmmmm', 'eeeeeeee', 'TTTTTTTTT', 'll', 'ooooooooooooooooooooooooooooo', 'tttttttt', 'LLLLLLLLL', 'mmmmmmmmmmmmmmm', 'AAA', 'xxxxx', 'rrrrrrrr', 'tttt', 'uu', 'uuuuuuuuuu', 'VVVVV', 'YY', '33', 'mmm', 'AAAAAAAAAA', 'TTT', 'YYYYY', 'OOOOOO', 'DDD', 'rr', 'PP', 'II', 'LLL', 'rrrrrr', 'ffff', 'GGGGGGGGGG', 'aaaaaaaaa', 'llllllllllllll', 'wwww', '________________________', 'ZZ', 'RRRRR', 'iii', 'mmmmmmmmmmmm', 'MMMMMMMMMMMMM', 'aa', 'MMMMMMMMMMMMMMMMM', 'rrr', '____', '000000000000', 'iiiiiiiii', 'ddddd', 'PPPPP', 'kkk', 'oo', 'TT', 'llllllllll', 'VVVVVVVVVV', 'mmmmmmm', 'llllllll', '__________', 'MMMMMMMM', 'mmmmm', 'ooooooooooooo', 'iiiiiii', '_______________________________________', 'vv', 'MMMMMM', 'ggggggggggggg', '_____', 'ggg', 'ddddddd', 'llllll', '111', 'wwwww', 'aaa', 'AAAAA', 'bbbb', '__', 'mmmmmmmm', 'aaaaaa', 'MMMMMMMMMMM', 'ssssssss', 'WWWWWW', '777', 'XX', 'yyyyyyyyyyy', 'yyyyyy', 'MM', 'EEEEE', 'iiiiiiii', 'hhhhhhhh', '________', 'aaaaaaaaaaaaa', 'www', 'BB', 'HHHHHHH', 'eee', 'IIIII', 'ppp', 'yyy', 'ooooooooooooooo', 'ssssss', 'SS', 'sssss', 'ww', 'pppp', '__________________________________________________________________________________', 'RRRRRRRRRRR', '77', 'vvvvv', 'wwwwwwwww', 'ff', 'mmmmmmmmmmm', 'AAAA', 'sss', 'aaaaaaaaaaaaaaa', 'oooooooo', 'RRRRRRR', 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', 'DDDDD', 'eeeeeeeeeee', 'ooooooooooo', 'mmmmmm', 'YYYYYY', 'NNNNNNNNNNNNNNNNNNNNNNNNNNN', 'MMM', 'MMMMM', 'wwwwwww', '_______________________________________________________________', 'oooo', 'aaaaaaaaaaaaaaaaaaaa', 'mmmmmmmmmmmmmm', '_____________________________________________________________________________', 'ooooo', 'eeeeeee', 'UUUUUUUUUUUU', 'VVVV', 'oooooo', 'NNNNN', 'cc', 'mmmmmmmmmmmmmmmmm', 'cccc', 'gggg', 'aaaa', 'FFFFFF', 'ttt', '11', '99', 'OOOOOOOOOO', 'ii', 'ooooooooooooooooo', 'yyyyyyyy', 'lll', 'uuuuuuu', 'EEEE', 'oooooooooooooo', 'UUUUU', 'HHHHH', 'wwwwww', 'gggggg', 'eeeeeeeeeeee', 'mmmmmmmmmmmmm', 'oooooooooooooooooooo', 'SSSSSSSSSSSSSSSSSSSS', 'ZZZZZ', 'ddddddddd', 'mmmm', 'YYY', 'MMMMMMM', 'kk', '88', 'ZZZZZZZ', 'nnnnnnn', '______________________________________', 'TTTTTTTT', '__________________________________________________________________________', 'mmmmmmmmmm', '333', 'zzzz', 'OOOOOOOOOOO', 'mmmmmmmmmmmmmmmm', 'xxxxxx', 'TTTT', '__________________________', 'jj', 'uuuuuuuu', 'xx', 'tttttt', '66666', 'ccccccccccc', 'MMMMMMMMMMMM', 'lllll', 'nn', 'uuuuuu', 'hh', 'AAAAAAA', 'xxxxxxx', 'OOOOOOOOOOOOOOO', 'EEEEEEEEE', 'UUUUUU', 'dddd', '1111111111', 'SSSSSSSSSSS', 'ddd', 'SSS', 'uuuuuuuuuuuuu', '________________', 'UUUUUUUUU', '__________________________________________________________________', '______________________________________________________________________________________', 'yy', 'ffffffff', 'LLLL', 'ttttttt', 'HHH', 'fffffff', 'cccccc', 'KK', 'NNN', 'MMMMMMMMM', '_______________', 'uuu', 'HH', 'ccc', 'DD', 'ooo', 'XXXX', 'yyyyy', 'UUUU', 'OOOOOOOOOOOOOOOOOOOO', 'ooooooooo', 'GGGG', 'MMMMMMMMMM', 'DDDDDDD', 'eeeeee', 'MMMMMMMMMMMMMMMMMMM', '0000', 'kkkkkkk', 'RRRR', 'zzzzzz', 'ppppp', 'xxx', 'AAAAAA', 'SSSS', 'zz', 'hhh', 'oooooooooooooooooooooo', 'EEE', '000000', '______________________________', 'vvv', 'KKKK', 'cccccccc', '55', 'zzzzzzz', 'eeeeeeeeeeeee', '00', 'ccccc', 'DDDDDDDD', 'iiiii', 'ttttt', 'ggggg', 'zzzzz', 'rrrrrrr', 'TTTTTTTTTTT', 'hhhh', 'FFFFF', '0000000', 'EE', 'GGGGG', 'ee', 'ZZZZ', 'EEEEEEE', 'ss', 'MMMM', 'WWW', 'uuuu', 'VV', 'aaaaaaa', 'vvvv', '00000', 'gg', '______________________________________________________________________________', 'dd', 'rrrr', 'kkkkk', '___________________________________', 'FF', 'AAAAAAAAA', 'pp', 'OOOOO', 'OOOOOOOO', 'LL', 'eeeeeeeeeeeeeeeeeeeeeee', 'XXX', '_______________________________________________________________________________________________________________', 'OOOOOOOOO', 'gggggggggg', 'YYYY', 'LLLLL', 'lllllll', 'TTTTT', 'EEEEEE', 'OOOOOOOOOOOOOOOOOOOOOOOOOOOOOO', '______', '000', 'eeee', 'UUUUUUUUUU', 'RRR', 'bb', 'aaaaaaaaaa', 'aaaaaaaaaaa', 'fffff', 'mmmmmmmmm', 'uuuuuuuuu', 'HHHHHH', 'NNNNNNNNNN', '______________', '_______', 'aaaaaaaa', 'UU', 'UUUUUUU', '66', 'SSSSSSS', 'ooooooooooooooooooooooooooo', 'NN', '9999', 'ttttttttttttttt', '___', 'yyyyyyyyy', 'CCC', '______________________________________________________________________________________________________________________', 'oooooooooooooooooo', 'TTTTTTTTTTTTTT', 'xxxx', 'yyyyyyyyyy', 'nnnnnn', 'yyyyyyy', 'IIII', 'ccccccc', '44', '__________________', 'YYYYYYYY', 'wwwwwwwwwwwwwww', 'iiiiii'}\n"
     ]
    }
   ],
   "source": [
    "# Finding special characters that are non alphanumeric and not spaces\n",
    "\n",
    "sp = set([char for char in text_old if not char.isalnum() and char != ' '])\n",
    "print(\"Special characters include:\",sp)\n",
    "\n",
    "# Findind words wih repeating characters \n",
    "rep = re.findall(r'(\\w)(\\1{2,})', text_old)\n",
    "rep = set([y for x, y in rep])\n",
    "print(\"Repeating characters include:\",rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first cleaning round \n",
    "\n",
    "def clean_1(review):\n",
    "    words = str(review).lower()  # lowercase all words\n",
    "    words = re.sub(r'[^A-Za-z ]+', ' ', words) # replace anything that is not alphabet with space \n",
    "    words = re.sub(r'(.)\\1+', r'\\1\\1', words) # turn\"soooo goooood\" into \"so good\"\n",
    "    return(words)\n",
    "\n",
    "data['clean1'] = data['reviewText'].apply(lambda x: clean_1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>department</th>\n",
       "      <th>clean1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>no adverse comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>gift for college student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>if you like strong tea  this is for you  it mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>love the tea  the flavor is way better than th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>i have searched everywhere until i browsed ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Tea made with Lipton Yellow Label teabags is m...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>tea made with lipton yellow label teabags is m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this tea!  Okay, I'm not a high falutin...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>i love this tea  okay  i m not a high falutin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Discovered this tea at a local Med. Rest....a ...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>discovered this tea at a local med  rest a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Well I bought this tea after being in Malaysia...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>well i bought this tea after being in malaysia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>We really like this tea.  It is definitely dif...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>we really like this tea  it is definitely diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Hard to find in the U.S.A.  Exactly as describ...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>hard to find in the u s a  exactly as describe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I like pretty much all of Lipton's tea... I ju...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>i like pretty much all of lipton s tea  i just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I was watching a youtube video about buying te...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>i was watching a youtube video about buying te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>it was ok, but it didn't taste like the Lipton...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>it was ok  but it didn t taste like the lipton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Great taste use it for cold brew - works great...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>great taste use it for cold brew  works great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Best tea for my single cup coffee maker. I pur...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>best tea for my single cup coffee maker  i pur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Good tea. Way better than baseline Lipton tea....</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>good tea  way better than baseline lipton tea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This tea looks like coffee grounds. Brewed it ...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>this tea looks like coffee grounds  brewed it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>OK tea.</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>ok tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My second favorite tea!  Drink lots it'll make...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>my second favorite tea  drink lots it ll make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall                                         reviewText department  \\\n",
       "0       5.0                                No adverse comment.  Beverages   \n",
       "1       5.0                          Gift for college student.  Beverages   \n",
       "2       5.0  If you like strong tea, this is for you. It mi...  Beverages   \n",
       "3       5.0  Love the tea. The flavor is way better than th...  Beverages   \n",
       "4       5.0  I have searched everywhere until I browsed Ama...  Beverages   \n",
       "5       4.0  Tea made with Lipton Yellow Label teabags is m...  Beverages   \n",
       "6       5.0  I love this tea!  Okay, I'm not a high falutin...  Beverages   \n",
       "7       5.0  Discovered this tea at a local Med. Rest....a ...  Beverages   \n",
       "8       4.0  Well I bought this tea after being in Malaysia...  Beverages   \n",
       "9       5.0  We really like this tea.  It is definitely dif...  Beverages   \n",
       "10      5.0  Hard to find in the U.S.A.  Exactly as describ...  Beverages   \n",
       "13      5.0  I like pretty much all of Lipton's tea... I ju...  Beverages   \n",
       "14      5.0  I was watching a youtube video about buying te...  Beverages   \n",
       "15      3.0  it was ok, but it didn't taste like the Lipton...  Beverages   \n",
       "16      5.0  Great taste use it for cold brew - works great...  Beverages   \n",
       "17      5.0  Best tea for my single cup coffee maker. I pur...  Beverages   \n",
       "18      4.0  Good tea. Way better than baseline Lipton tea....  Beverages   \n",
       "19      1.0  This tea looks like coffee grounds. Brewed it ...  Beverages   \n",
       "20      5.0                                            OK tea.  Beverages   \n",
       "21      5.0  My second favorite tea!  Drink lots it'll make...  Beverages   \n",
       "\n",
       "                                               clean1  \n",
       "0                                 no adverse comment   \n",
       "1                           gift for college student   \n",
       "2   if you like strong tea  this is for you  it mi...  \n",
       "3   love the tea  the flavor is way better than th...  \n",
       "4   i have searched everywhere until i browsed ama...  \n",
       "5   tea made with lipton yellow label teabags is m...  \n",
       "6   i love this tea  okay  i m not a high falutin ...  \n",
       "7   discovered this tea at a local med  rest a gre...  \n",
       "8   well i bought this tea after being in malaysia...  \n",
       "9   we really like this tea  it is definitely diff...  \n",
       "10  hard to find in the u s a  exactly as describe...  \n",
       "13  i like pretty much all of lipton s tea  i just...  \n",
       "14  i was watching a youtube video about buying te...  \n",
       "15  it was ok  but it didn t taste like the lipton...  \n",
       "16  great taste use it for cold brew  works great ...  \n",
       "17  best tea for my single cup coffee maker  i pur...  \n",
       "18  good tea  way better than baseline lipton tea ...  \n",
       "19  this tea looks like coffee grounds  brewed it ...  \n",
       "20                                            ok tea   \n",
       "21  my second favorite tea  drink lots it ll make ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters include: set()\n",
      "Repeating characters include: {'yy', 'll', 'mmm', 'ss', 'ff', 'oo', 'aa', 'ww', 'aaa', 'ii', 'tt', 'ee', 'mm', 'dd'}\n"
     ]
    }
   ],
   "source": [
    "# check after cleaning \n",
    "\n",
    "clean = \"\".join(r for r in data['clean1'])\n",
    "\n",
    "sp = set([char for char in clean if not char.isalnum() and char != ' '])\n",
    "print(\"Special characters include:\",sp)\n",
    "\n",
    "# Findind words wih repeating characters \n",
    "rep = re.findall(r'(\\w)(\\1{2,})', clean)\n",
    "rep = set([y for x, y in rep])\n",
    "print(\"Repeating characters include:\",rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Old Review -\n",
      "GREAT PRODUCT, FAST DELIVERY, THANK YOU WILL ORDER AGAIN SOON .\n",
      "\n",
      "- New Review -\n",
      "great product  fast delivery  thank you will order again soon  \n"
     ]
    }
   ],
   "source": [
    "print(\"- Old Review -\")\n",
    "print(data['reviewText'][7102])\n",
    "print(\"\\n- New Review -\")\n",
    "print(data['clean1'][7102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tokenized column\n",
    "\n",
    "data['tokens'] = data['clean1'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words before lower case, special characters, and numbers removal:  222703\n",
      "Number of unique words after lower case, special characters, and numbers removal:  98785\n"
     ]
    }
   ],
   "source": [
    "# Reviews before lower case, special characters, and numbers removal \n",
    "w_lists = [word_tokenize(r) for r in data['reviewText']]\n",
    "words = [w for review in w_lists for w in review]\n",
    "prev = len(set(words))\n",
    "print(\"Number of unique words before lower case, special characters, and numbers removal: \",prev)\n",
    "\n",
    "# Reviews after lower case, special characters, and numbers removal \n",
    "words_clean = [w for review in data['tokens'] for w in review]\n",
    "post = len(set(words_clean))\n",
    "print(\"Number of unique words after lower case, special characters, and numbers removal: \", post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.36 % of word were removed after lower case, special characters, and numbers removal\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words before lower case, special characters, and numbers removal:  222,703\n",
    "# Number of unique words after lower case, special characters, and numbers removal:  98,785\n",
    "print(\"{:.2f}\".format(((post/prev)*100)), '% of word were removed after lower case, special characters, and numbers removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Words and Remove Stopwords\n",
    "\n",
    "Stopwords are commonly used words in any language. For example, some stopwords in English include \"the\", \"her\", \"can\". While these words are important for sentence structures, they do not carry a lot of meaning in our analysis. Therefore, they will be removed from the reviews. \n",
    "\n",
    "Different forms of a word often convey the same meaning. For example, \"boxes\" and \"box\" essentially represent the same object. The same word may also appear in different tenses (i.e. \"look\", \"looked\", and \"looking\"). These different forms of the word are known as **inflections**. In general, inflections should be analyzed as a single term since they represent the same meaning. \n",
    "\n",
    "In the following section, each word in the review will be tagged for its respectice Part of Speech (POS), i.e. noun, verb, adjective, or adjective. Each word in the reviews will be tokenized. Then the nltk pos_tag library will be used to identify POS of each word. \n",
    "\n",
    "After that, the process of lemmatization is applied using the nltk wordnet library. Lemmatization takes into consideration the morphological analysis of the words. So lemmatization considers the grammar of the word and tries to find the root word instead of just getting to the root word by brute force methods.\n",
    "\n",
    "\n",
    "For more information on lemmatization, please visit the below resource:\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'amazon',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'could',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'even',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'get',\n",
       " 'go',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'make',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'nor',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'use',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of English stopwords to remove, excluding negative words since they will give us weights in bigrams \n",
    "\n",
    "remove_from_stop = [\"not\", \"don't\", \"no\"]\n",
    "add_to_stop = [\"one\", \"get\", \"go\", \"even\", \"amazon\", \"could\", \"would\", \"use\", \"make\"]\n",
    "stopwords = set(stopwords.words('english')).union(set(add_to_stop)) - set(remove_from_stop)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UlnxCmOKQ3U"
   },
   "outputs": [],
   "source": [
    "#Second round of cleaning: initialize lemmatizer and remove stop words \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"Map POS tag to first character lemmatize() accepts\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    if tag == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_sw_removal(review):\n",
    "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in review]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    words = \" \".join(w for w in words)\n",
    "    return(words)\n",
    "\n",
    "data['clean2'] = pd.DataFrame(data['tokens'].apply(lambda x: lemma_sw_removal(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "opzXcy7QGSMW",
    "outputId": "a7e9ec3c-0189-4901-f8df-645d4a1b48e7"
   },
   "outputs": [],
   "source": [
    "data[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the empty cells after cleaning\n",
    "\n",
    "print(\"There are:\", len(data[data['clean2'] == ''].index), \"empty reviews after pre-processing\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace empty cells with NaN values \n",
    "data['clean2'].replace('', np.nan, inplace=True)\n",
    "\n",
    "#drop rows with NaN values \n",
    "data.dropna(subset=['clean2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"clean_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataCleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
